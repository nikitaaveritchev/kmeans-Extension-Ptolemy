\newcommand{\kmeans}{$k$-means problem\xspace}

\section{Preliminaries}

\lipsum[23]
\todo{explain structure in 3 sentences}


\subsection{The \kmeans}

To discuss the \kmeans, we first need to formalize the concept of a \emph{clustering algorithm}:
\begin{definition}[clustering algorithm]
	Given a target number of clusters $k$,
  let $F$ be a function that assigns each element from an input data set $D\subset \mathbb{R}^n$ to a
	cluster index $i \in \{1, 2, \ldots, k\}$
	$$ F:D \to \{1, 2, \ldots, k\} \;.$$
\end{definition}
Notably, we restrict our analysis to clustering problems that use the $n$-dimensional Euclidean space as the domain of the input data set.
Likewise, we assume that the similarity of the data points to be clustered is completely defined by the Euclidean distance $d(x,y)= || x-y ||_2 $.
The rationale behind this assumption will be made clear shortly in \autoref{sub:acc}.

The heart of the \kmeans is the assignment of clusters so that the variance in similarity between all points in the same cluster should be reasonably small.
More formally:
\begin{definition}[\kmeans]
	Let $F$ be a clustering algorithm
	%$x \in D$ be an element from the input data set of a clustering algorithm $F$
	and let $C_i$ be the set of all elements assigned to cluster $i$:
	$$ C_i = \{x \in D \mid F(x) = i\} \,.$$
	Furthermore, let $\epsilon$ be the weighted sum of inter-cluster variances
	$$ \epsilon = \sum_{i=1}^k |C_i| \operatorname{Var}[C_i] = \sum_{i=1}^k \sum_{x_j \in C_i}  d^2(x_j, c_i)\,, $$
	where $c_i = \operatorname{E}[C_i]$ is the mean over all points in the cluster.

	Then, the map $F$ is called a solution to the \kmeans
	if $\epsilon$ cannot be made smaller by changing the assignment of any one element to a different cluster.
\end{definition}
Note that with this definition, the common $k$-means algorithms, like Lloyd's, provides solution to the \kmeans,
as we do not require that $F$ finds the smallest possible $\epsilon$,
just a local minimum of $\epsilon$.
This is consistent with the colloquial notion that solutions to the \kmeans ``can be found'',
even when an optimal solution (an NP-hard problem) is not practically obtainable \cite{}.

The most prevalent algorithm employed to identify valid solutions to the \kmeans is the Lloyd algorithm.
For convenience, it is reproduced in \autoref{alg:lloyd};
given its widespread use, we will only cover its most significant characteristics as they relate to our analysis.

\begin{algorithm}
	\caption{k-Means Algorithm}
	\label{alg:lloyd}

	\textbf{Input:} \( k \): Number of clusters, \( D \): Dataset containing \( n \) objects

	\textbf{Output:} Assignment of each $x_i \in D$ to a clusters

	\begin{algorithmic}[1]
		\State Initialize \( k \) cluster centers \( \{c_1, c_2, \dots, c_k\} \) arbitrarily from \( D \)
		\Repeat
		\label{algstep:assign}
		\State \textcolor{gray}{// Assign \( x_i \) to the closest cluster}
		\For{each object \( x_i \) in \( D \)}
		\State \( C_j \leftarrow C_j \cup \{x_i\} \) where \( j = \underset{j}{\arg\min} \; d^2(x_i, c_j) \)
		\EndFor
		\State \textcolor{gray}{// Update cluster center \( c_j \)}
		\For{each cluster \( C_j \)}
		\State \( c_j \leftarrow \frac{1}{|C_j|} \sum_{x_i \in C_j} x_i \)
		\EndFor
		\Until{no change in cluster assignments}
	\end{algorithmic}
\end{algorithm}

For each iteration of Lloyds algorithm, the assignment step (\autoref{algstep:assign}) needs to identify the closest cluster center for each element from the input dataset $D$.
To do so, Lloyd's algorithm calculates the distances between each cluster and each element from $D$, requiring $k\cdot|D|$ total distance evaluations per iteration.
These frequent distance evaluations can make up a significant share of the algorithms computational costs, especially for high-dimensional datasets.


\subsection{Accelerating $k$-means}
\label{sub:acc}

As we have seen, Lloyd's algorithm requires numerous evaluations of the distance function.
However, the exact distances do not actually need to be calculated explicitly for every element, as it is sufficient to identify which cluster center is the closest to a given element.
This is often possible from geometric considerations alone, when lower and upper distance bounds are available.
Two properties of the Euclidean space are of particular use here:
The triangle inequality
\begin{align}
  \label{eq:tri}
  d(x,y) \leq d(x,z) + d(z,y)
\end{align}
and Ptolemy's inequality
\begin{align}
  \label{eq:pto}
  d(x, y)\cdot d(v, u) \leq d(x, v) \cdot d(y,v) + d(x, u) \cdot d(y, v)
\end{align}
can provide lower and upper bound on the distance between two points given two (respectively five) other distances.

Numerous algorithms have been developed that exploit the former inequality to avoid explicit distance evaluations. However, to the best of our knowledge, the latter has not been employed for the purposes of accelerating k-means clustering yet.

Therefore, we define Elkan's algorithm \cite{}, which only uses the triangle inequality, here and extend it by incorporating Ptolemy's inequality in \ref{}.


\begin{align}
  \label{eq:elkan_lower}
  l(x_i, c_j) &= \max \{ l(x_i, c_j) - d(c'_j, c_j), 0 \} \\
  \label{eq:elkan_upper}
  u(x_i) &= u(x_i) + d(c'(x_i), c(x_i))
\end{align}

\input{content/alg_elkan2.tex}



\todo{explain Hetland's lower (upper) bound notation ($d^-$ and $d^+$)}
\begin{theorem}[Ptolemaic Bounds]
	\label{thm4.2}
	\todo{we might want to explain what the points are supposed to do before showing this theorem.
		Or, alternatively, we just present this as a purely mathematical statement with no preceding motivation.}
	Let \(a, b, c, c' \in \mathbb{R}^n\) and let \(d(\cdot, \cdot)\) be the Euclidean distance. The distance \(d(a, c')\) can be bounded as follows:
	\begin{gather}
		\frac{1}{d(b, c)} \cdot \max \left\{
		\begin{array}{l}
			d(a, b)^- \cdot d(c, c') - d(a, c)^+ \cdot d(b, c'), \\
			d(a, c)^- \cdot d(b, c') - d(a, b)^+ \cdot d(c, c')
		\end{array}
		\right\} \leq d(a, c')
	\end{gather}
	and
	\begin{gather}
		d(a, c') \leq \frac{1}{d(b, c)} \cdot \left( d(a, c) \cdot d(b, c') + d(a, b) \cdot d(c, c') \right)
	\end{gather}
\end{theorem}
\begin{proof}
	\todo{convert to passive voice}
	\todo{expand context to summarize all steps that lead to this proof}
	The lower bound corresponds to the derivation in equation \ref{eq4.4} and corresponds to the arrangement of the two pivot objects. Replace some distances with the respective lower or upper bound for the distance. The lower bounds can be used as they are always smaller than the exact distance. The upper bound must be used due to the subtraction. This means that the inequality still applies. The upper bound is obtained by using Ptolemy's inequality:

	\begin{equation*}
		\begin{aligned}
			\label{eq4.7}
			d(a,c') \cdot d(b,c) & \leq d(a, c) \cdot d(b, c') + d(a, b) \cdot d(c, c')                \\
			d(a,c')              & \leq \frac{d(a, c) \cdot d(b, c') + d(a, b) \cdot d(c, c')}{d(b,c)}
		\end{aligned}
	\end{equation*}
	By replacing some distances with upper boundaries you get the desired upper boundaries.
\end{proof}




\section{Applying Ptolemy's Inequality to the Elkan's Algorithm}




\subsection{Integration in k-Means}
Theorem \ref{thm4.2} will now be applied in a similar context to Elkan's algorithm \ref{alg:elkan}, with the goal of integrating these new bounds into steps 5 and 6 of the algorithm.

Consider the data set \(D = \{x_1, \ldots, x_n\}\). For each \(x_i\), there is an upper \(u_i\) and lower bounds \(l_{ij}\) as in Elkan's algorithm. The new cluster center assigned in a iteration is denoted by \(c_i^{\text{new}}\), and the old cluster center by \(c_i^{\text{old}}\). To use Ptolemy's Inequality, one additional point is needed. The idea is to use an even older cluster center \(c_i^{\text{old,old}}\) obtained two iterations ago.

Overall, to obtain a new upper bound for the distance between the datapoint \(x_i\) and the new cluster center \(c_j^{\text{new}}\), consider the old cluster center \(c_j^{\text{old}}\) from the last iteration to which \(x_i\) was assigned, and the even older cluster center \(c_j^{\text{old,old}}\) from two iterations ago. Utilize the old upper bounds that were valid for the respective iteration.

\begin{equation*}
	\begin{aligned}
		d(x_i,c_i^{old})      & \leq u_i^{old}      \\
		d(x_i,c_i^{old, old}) & \leq u_i^{old, old}
	\end{aligned}
\end{equation*}

Similarly, to obtain a new lower bound for the distance between the datapoint \(x_i\) and the new cluster center \(c_j^{\text{new}}\), the old lower bounds from the respective iterations are used:

\begin{equation*}
	\begin{aligned}
		l_{i,j}^{\text{old}}     & \leq d(x_i, c_j^{\text{old}})     \\
		l_{i,j}^{\text{old,old}} & \leq d(x_i, c_j^{\text{old,old}})
	\end{aligned}
\end{equation*}

Using Theorem \ref{thm4.2}, the following upper bound for the distance are obtained:
\begin{equation}
	\label{eq4.7}
	d(x_i, c_j^{\text{new}}) \leq u_i^{\text{new}} = \frac{1}{d(c_j^{\text{old}}, c_j^{\text{old,old}})} \cdot \left( u_i^{\text{old}} \cdot d(c_j^{\text{new}}, c_j^{\text{old,old}}) + u_i^{\text{old,old}} \cdot d(c_j^{\text{new}}, c_j^{\text{old}}) \right)
\end{equation}

and the new lower bound:
\begin{equation}
	\label{eq4.8}
	d(x_i, c_j^{\text{new}}) \geq l_{i,j}^{\text{new}} = \frac{1}{d(c_j^{\text{old}}, c_j^{\text{old,old}})} \cdot \max \left\{
	\begin{array}{l}
		l_{i,j}^{\text{old,old}} \cdot d(c_j^{\text{old}}, c_j^{\text{new}}) - u_i^{\text{old}} \cdot d(c_j^{\text{new}}, c_j^{\text{old,old}}), \\
		l_{i,j}^{\text{old}} \cdot d(c_j^{\text{new}}, c_j^{\text{old,old}}) - u_i^{\text{old,old}} \cdot d(c_j^{\text{old}}, c_j^{\text{new}})
	\end{array}
	\right\}
\end{equation}

You can see that the old upper boundaries are required to calculate the new lower boundaries.

Let us now look at how these bounds can be integrated into Elkan's algorithm. It is important to save not only the cluster centres and the bounds of the last iteration, but also those of the penultimate iteration.

At the beginning, all distances to all cluster centres are calculated in the first iteration. The upper bound is set by the distance to the nearest cluster centre, and the lower bounds are determined by the direct distances from the point to the respective cluster centres. An iteration is then performed as with Elkan, which means that bounds from two iterations are now available. This makes it possible to apply the new bounds from equations \ref{eq4.7} and \ref{eq4.8}. The algorithm then iterates, as in the original Elkan algorithm using updating with new update conditions, until the cluster centres converge or a specified number of iterations is reached. It is always important to check whether $d(c_j^{\text{old}}, c_j^{\text{old,old}}) \neq 0$. If this is the case, the update is carried out as for Elkan. The algorithm is displayed in Algorithm \ref{alg:ptolemy}.





