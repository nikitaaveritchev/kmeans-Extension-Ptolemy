{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bachlor Thesis: Extending K-Means Clustering with Ptolemyâ€™s Inequality\n",
    "## Performance Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "In this notebook, we evaluate the performance of the proposed K-Means Extension with Ptolemy's Inequality by applying it to various synthetic and real-world datasets. \n",
    "Several versions of the K-Means clustering algorithm are compared:\n",
    "- **Classical Lloyd's K-Means**\n",
    "- **Elkan's Algorithm**\n",
    "- **Elkan's Algorithm using Ptolemy for Upper Bounds**\n",
    "- **Elkan's Algorithm using Ptolemy for Lower Bounds**\n",
    "- **Elkan's Algorithm using Ptolemy for Upper and Lower Bounds**\n",
    "\n",
    "These algorithms are tested on the following datasets:\n",
    "- **Gaussian Blobs**: Synthetic dataset with isotropic Gaussian blobs, useful for testing clustering algorithms based on Euclidean distance.\n",
    "- **Iris**: Classic dataset containing measurements of iris flowers, with three different species.\n",
    "- **Wine**: Dataset with chemical analysis results of wines grown in the same region in Italy, but derived from three different cultivars.\n",
    "- **Moons**: Synthetic dataset with two interleaving half circles, demonstrating the need for clustering algorithms to handle non-linear separations.\n",
    "- **Circles**: Synthetic dataset with a large circle containing a smaller circle, testing the algorithm's ability to manage concentric shapes.\n",
    "- **Classification**: Generated dataset with clusters, providing control over feature distribution and separation between classes.\n",
    "- **Sparse Blobs**: High-dimensional synthetic dataset with sparse data points, challenging the algorithm's efficiency in higher dimensions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and imports\n",
    "Here we import our custom K-Means algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from means import Kmeans, DataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification, make_gaussian_quantiles, load_iris, load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets():\n",
    "    datasets = {}\n",
    "\n",
    "    # Gaussian (Blobs)\n",
    "    low_dim_generator = DataGenerator(n_samples=10000, n_features=10, n_clusters=4, random_state=43)\n",
    "    low_dim_data, low_dim_labels = low_dim_generator.generate_data()\n",
    "    datasets[\"Gaussian_low\"] = low_dim_data\n",
    "\n",
    "    medium_dim_generator = DataGenerator(n_samples=10000, n_features=100, n_clusters=4, random_state=43)\n",
    "    medium_dim_data, medium_dim_labels = medium_dim_generator.generate_data()\n",
    "    datasets[\"Gaussian_medium\"] = medium_dim_data\n",
    "\n",
    "    high_dim_generator = DataGenerator(n_samples=10000, n_features=300, n_clusters=4, random_state=43)\n",
    "    high_dim_data, high_dim_labels = high_dim_generator.generate_data()\n",
    "    datasets[\"Gaussian_high\"] = high_dim_data\n",
    "\n",
    "    # Iris\n",
    "    iris = load_iris()\n",
    "    X_iris = iris.data\n",
    "    datasets['Iris'] = X_iris\n",
    "\n",
    "    # Wines\n",
    "    wine = load_wine()\n",
    "    X_wine = wine.data\n",
    "    datasets['Wine'] = X_wine\n",
    "\n",
    "    # Moons\n",
    "    X_moons, _ = make_moons(n_samples=10000, noise=0.1, random_state=42)\n",
    "    datasets['Moons'] = X_moons\n",
    "\n",
    "    # Circles\n",
    "    X_circles, _ = make_circles(n_samples=15000, noise=0.1, factor=0.5, random_state=42)\n",
    "    datasets['Circles'] = X_circles\n",
    "\n",
    "    # Random\n",
    "    datasets[\"Random_low\"] = np.random.uniform(0, 10, size=(5000, 10))  # Low dimension random dataset\n",
    "    datasets[\"Random_medium\"] = np.random.uniform(0, 10, size=(5000, 100))  # Medium dimension random dataset\n",
    "    datasets[\"Random_high\"] = np.random.uniform(0, 10, size=(5000, 300))  # High dimension random dataset\n",
    "\n",
    "    return datasets\n",
    "\n",
    "datasets = generate_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store results\n",
    "results = {\n",
    "    'Dataset': [],\n",
    "    'Method': [],\n",
    "    'K': [],\n",
    "    'Distance_Evaluations': []\n",
    "}\n",
    "\n",
    "# List of methods to test\n",
    "methods = ['classic', 'Elkan', 'Ptolemy_upper', 'Ptolemy_lower', 'Ptolemy']\n",
    "# List of k values to test\n",
    "k_values = [3, 20, 100]\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    print(f\"Processing dataset: {name}\")\n",
    "    \n",
    "    for k in k_values:\n",
    "        for method in methods:\n",
    "            # Initialize and fit the model\n",
    "            model = Kmeans(k=k, method=method)\n",
    "            model.fit(data)\n",
    "            \n",
    "            # Store results\n",
    "            results['Dataset'].append(name)\n",
    "            results['Method'].append(method)\n",
    "            results['K'].append(k)\n",
    "            results['Distance_Evaluations'].append(model.distance_evaluations)  # Accessing distance evaluations\n",
    "        \n",
    "            # Print results\n",
    "            print(f\"{method} (k={k}) - Distance evaluations: {model.distance_evaluations}\")\n",
    "\n",
    "# Convert the results dictionary to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results DataFrame\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the following structures exist\n",
    "results = {\n",
    "    'Dataset': [],\n",
    "    'Method': [],\n",
    "    'K': [],\n",
    "    'Distance_Evaluations': []\n",
    "}\n",
    "methods = ['classic', 'Elkan', 'Ptolemy_upper', 'Ptolemy_lower', 'Ptolemy'] # Example methods\n",
    "k_values = [3, 20, 100]  # Example k values\n",
    "\n",
    "def process_dataset(name, data, k_values, methods):\n",
    "    print(f\"Processing dataset: {name}\")\n",
    "    \n",
    "    for k in k_values:\n",
    "        for method in methods:\n",
    "            # Initialize and fit the model\n",
    "            model = Kmeans(k=k, method=method)\n",
    "            model.fit(data)\n",
    "            \n",
    "            # Store results\n",
    "            results['Dataset'].append(name)\n",
    "            results['Method'].append(method)\n",
    "            results['K'].append(k)\n",
    "            results['Distance_Evaluations'].append(model.distance_evaluations)\n",
    "        \n",
    "            # Print results\n",
    "            print(f\"{method} (k={k}) - Distance evaluations: {model.distance_evaluations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Random_low\n",
      "classic (k=3) - Distance evaluations: 840000\n",
      "Elkan (k=3) - Distance evaluations: 879890\n",
      "Ptolemy_upper (k=3) - Distance evaluations: 869932\n",
      "Ptolemy_lower (k=3) - Distance evaluations: 882837\n",
      "Ptolemy (k=3) - Distance evaluations: 872355\n",
      "classic (k=20) - Distance evaluations: 6000000\n",
      "Elkan (k=20) - Distance evaluations: 10330209\n",
      "Ptolemy_upper (k=20) - Distance evaluations: 2863457\n",
      "Ptolemy_lower (k=20) - Distance evaluations: 11052738\n",
      "Ptolemy (k=20) - Distance evaluations: 2964126\n",
      "classic (k=100) - Distance evaluations: 23500000\n",
      "Elkan (k=100) - Distance evaluations: 11836355\n",
      "Ptolemy_upper (k=100) - Distance evaluations: 2284908\n",
      "Ptolemy_lower (k=100) - Distance evaluations: 20236336\n",
      "Ptolemy (k=100) - Distance evaluations: 3996726\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'Random_low'\n",
    "dataset_data = datasets[dataset_name]\n",
    "\n",
    "process_dataset(dataset_name, dataset_data, k_values, methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Random_medium\n",
      "classic (k=3) - Distance evaluations: 1080000\n",
      "Elkan (k=3) - Distance evaluations: 1110000\n",
      "Ptolemy_upper (k=3) - Distance evaluations: 1102015\n",
      "Ptolemy_lower (k=3) - Distance evaluations: 1110000\n",
      "Ptolemy (k=3) - Distance evaluations: 1094784\n",
      "classic (k=20) - Distance evaluations: 7900000\n",
      "Elkan (k=20) - Distance evaluations: 3300000\n",
      "Ptolemy_upper (k=20) - Distance evaluations: 2504815\n",
      "Ptolemy_lower (k=20) - Distance evaluations: 3300000\n",
      "Ptolemy (k=20) - Distance evaluations: 2146340\n",
      "classic (k=100) - Distance evaluations: 9000000\n",
      "Elkan (k=100) - Distance evaluations: 7499766\n",
      "Ptolemy_upper (k=100) - Distance evaluations: 5660515\n",
      "Ptolemy_lower (k=100) - Distance evaluations: 7499786\n",
      "Ptolemy (k=100) - Distance evaluations: 4391286\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'Random_medium'\n",
    "dataset_data = datasets[dataset_name]\n",
    "\n",
    "process_dataset(dataset_name, dataset_data, k_values, methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Random_high\n",
      "classic (k=3) - Distance evaluations: 525000\n",
      "Elkan (k=3) - Distance evaluations: 645000\n",
      "Ptolemy_upper (k=3) - Distance evaluations: 645000\n",
      "Ptolemy_lower (k=3) - Distance evaluations: 645000\n",
      "Ptolemy (k=3) - Distance evaluations: 645000\n",
      "classic (k=20) - Distance evaluations: 3100000\n",
      "Elkan (k=20) - Distance evaluations: 2200000\n",
      "Ptolemy_upper (k=20) - Distance evaluations: 2130270\n",
      "Ptolemy_lower (k=20) - Distance evaluations: 2200000\n",
      "Ptolemy (k=20) - Distance evaluations: 1638100\n",
      "classic (k=100) - Distance evaluations: 7000000\n",
      "Elkan (k=100) - Distance evaluations: 6934999\n",
      "Ptolemy_upper (k=100) - Distance evaluations: 6330177\n",
      "Ptolemy_lower (k=100) - Distance evaluations: 6934999\n",
      "Ptolemy (k=100) - Distance evaluations: 3794620\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'Random_high'\n",
    "dataset_data = datasets[dataset_name]\n",
    "\n",
    "process_dataset(dataset_name, dataset_data, k_values, methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Export to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate speedup for each variant compared to Elkan\n",
    "def calculate_speedup(df):\n",
    "    speedup_results = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['Method'] == 'Elkan':\n",
    "            speedup_results.append(1.0)  # Speedup is 1 for the classic method itself\n",
    "        else:\n",
    "            classic_dist_evals = df[(df['Dataset'] == row['Dataset']) & (df['K'] == row['K']) & (df['Method'] == 'Elkan')]['Distance_Evaluations'].values[0]\n",
    "            speedup = classic_dist_evals / row['Distance_Evaluations']\n",
    "            speedup_results.append(speedup)\n",
    "    return speedup_results\n",
    "\n",
    "results_df['Speedup'] = calculate_speedup(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_excel('clustering_performance_results2.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clustering_performance_results_formatted2.xlsx'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill, Font, Alignment\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'clustering_performance_results2.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Rearrange columns if needed (currently they seem to be in a logical order)\n",
    "# e.g., df = df[['Dataset', 'K', 'Method', 'Distance_Evaluations', 'Speedup']]\n",
    "\n",
    "# Save the DataFrame back to an Excel file with formatting\n",
    "output_file_path = 'clustering_performance_results_formatted2.xlsx'\n",
    "df.to_excel(output_file_path, index=False, sheet_name='Results')\n",
    "\n",
    "# Load the workbook and the sheet\n",
    "wb = load_workbook(output_file_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Apply formatting\n",
    "for col in ws.columns:\n",
    "    max_length = 0\n",
    "    column = col[0].column_letter  # Get the column name\n",
    "    for cell in col:\n",
    "        try:\n",
    "            if len(str(cell.value)) > max_length:\n",
    "                max_length = len(cell.value)\n",
    "        except:\n",
    "            pass\n",
    "    adjusted_width = (max_length + 2)\n",
    "    ws.column_dimensions[column].width = adjusted_width\n",
    "\n",
    "# Define styles\n",
    "header_font = Font(bold=True)\n",
    "center_aligned_text = Alignment(horizontal=\"center\")\n",
    "green_fill = PatternFill(start_color=\"C6EFCE\", end_color=\"C6EFCE\", fill_type=\"solid\")\n",
    "red_fill = PatternFill(start_color=\"FFC7CE\", end_color=\"FFC7CE\", fill_type=\"solid\")\n",
    "\n",
    "# Apply styles to header\n",
    "for cell in ws[1]:\n",
    "    cell.font = header_font\n",
    "    cell.alignment = center_aligned_text\n",
    "\n",
    "# Apply conditional formatting for Speedup column\n",
    "for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=5, max_col=5):\n",
    "    for cell in row:\n",
    "        if cell.value > 1:\n",
    "            cell.fill = green_fill\n",
    "        elif cell.value < 1:\n",
    "            cell.fill = red_fill\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(output_file_path)\n",
    "\n",
    "output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dim_generator = DataGenerator(n_samples=10000, n_features=10, n_clusters=4, random_state=43)\n",
    "low_dim_data, low_dim_labels = low_dim_generator.generate_data()\n",
    "\n",
    "model1 = Kmeans(k=4, method='classic')\n",
    "model1.fit(low_dim_data)\n",
    "print(\"Elkan\")\n",
    "model2 = Kmeans(k=4, method='Elkan')\n",
    "model2.fit(low_dim_data)\n",
    "print(\"Ptolemy_upper\")\n",
    "model3 = Kmeans(k=4, method='Ptolemy_upper')\n",
    "model3.fit(low_dim_data)\n",
    "print(\"Ptolemy_lower\")\n",
    "model4 = Kmeans(k=4, method='Ptolemy_lower')\n",
    "model4.fit(low_dim_data)\n",
    "print(\"Ptolemy\")\n",
    "model5 = Kmeans(k=4, method='Ptolemy')\n",
    "model5.fit(low_dim_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to plot clusters\n",
    "def plot_clusters(data, centroids, labels, title, ax):\n",
    "    unique_labels = np.unique(labels)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "    for label, color in zip(unique_labels, colors):\n",
    "        points = data[labels == label]\n",
    "        ax.scatter(points[:, 0], points[:, 1], s=30, color=color, label=f'Cluster {label+1}')\n",
    "    \n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1], s=200, color='red', marker='X', label='Centroids')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "# Assuming low_dim_data is your dataset and model1, model2, model3 are your fitted models\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(18, 6))\n",
    "\n",
    "plot_clusters(low_dim_data, np.array(list(model1.centroids.values())), np.array(model1.labels), 'Classic Method', axs[0])\n",
    "plot_clusters(low_dim_data, np.array(list(model2.centroids.values())), np.array(model2.labels), 'Elkan Method', axs[1])\n",
    "plot_clusters(low_dim_data, np.array(list(model3.centroids.values())), np.array(model3.labels), 'Ptolemy Method', axs[2])\n",
    "plot_clusters(low_dim_data, np.array(list(model4.centroids.values())), np.array(model4.labels), 'Ptolemy Method', axs[3])\n",
    "plot_clusters(low_dim_data, np.array(list(model5.centroids.values())), np.array(model5.labels), 'Ptolemy Method', axs[4])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Classic Distance evaluations: \" + str(model1.distance_evaluations))\n",
    "print(\"Elkan Distance evaluations: \" + str(model2.distance_evaluations))\n",
    "print(\"Ptolemy Distance evaluations: \" + str(model3.distance_evaluations))\n",
    "print(\"Ptolemy Distance evaluations: \" + str(model4.distance_evaluations))\n",
    "print(\"Ptolemy Distance evaluations: \" + str(model5.distance_evaluations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dim_generator = DataGenerator(n_samples=1000, n_features=10, n_clusters=4, random_state=43)\n",
    "low_dim_data, low_dim_labels = low_dim_generator.generate_data()\n",
    "\n",
    "model1 = Kmeans(k=4, method='Ptolemy')\n",
    "model1.fit(low_dim_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
